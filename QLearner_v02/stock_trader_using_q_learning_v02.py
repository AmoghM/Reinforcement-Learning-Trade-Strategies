# -*- coding: utf-8 -*-
"""Nov-Dec

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yWVJpo2nne7N2jla67bPQGOObI8sKghy
"""

import datetime
import numpy as np
import pandas  as pd
import matplotlib.pyplot as plt
#import data_process as d
import pandas_datareader.data as web
import time

"""#Import data

1. Get the state for cash (0 - 180K:9, > 180K:1) 
2. Get the state for shares (0  - 504:9 >300: 1) 
3. During training
4. BB band - using 2SD
5. Fixing alpha small. (choice made) Decay by episode by 1%. (Reconsider the math in the future in case inedequate)
6. Re-code epslion seperate from alpha. Start with high value eg. 99%
7. Window  ideally 20 days (aka 1 month) look back. Use an exponential moving average.
"""

def get_stock_data(symbol, start, end, train_size=0.8):
    '''
    Get stock data in the given date range
    Inputs:
    symbol(str): stock symbol
    start(datetime): start date
    end(datetime): end date
    train_size(float): amount of data used for training
    Outputs:
    train_df, test_df OR df(if train_size=1)
    '''
    df = web.DataReader(symbol, 'yahoo', start, end)
    return df

start = datetime.datetime(2007, 1, 1)
end = datetime.datetime(2016, 12, 31)
start_1 = datetime.datetime(2017, 1, 1)
end_1 = datetime.datetime(2019, 12, 31)

train_df = get_stock_data('JPM', start, end, 1)
test_df = get_stock_data('JPM', start_1, end_1, 1)

train_df.head()

test_df.head()

all_actions = {0:'hold', 1:'buy', 2:'sell'}

# def get_bollinger_bands(values, window):
#     '''
#     Return upper and lower Bollinger Bands.
#     INPUTS:
#     values(pandas series)
#     window(int): time period to consider
#     OUTPUS:
#     band_width(pandas series)
#     '''
#     #  rolling mean
#     rm = values.rolling(window=window).mean()
#     rstd = values.rolling(window=window).std()
    
#     band_width = 2. * rstd / rm
#     return band_width.apply(lambda x: round(x,5))


def get_upper_lower_bands(values, window):
    
    upper = values.rolling(window = window).mean() + values.rolling(window = window).std() * 2
    lower = values.rolling(window = window).mean() - values.rolling(window = window).std() * 2

    upper = upper.apply(lambda x: round(x,5))
    lower = lower.apply(lambda x: round(x,5))

    return upper, lower

def difference(list1, list2):
  difference = []
  zip_object = zip(list1, list2)
  for list1_i, list2_i in zip_object:
    difference.append(list1_i-list2_i) 
  return difference 

def get_adj_close_sma_ratio(values, window):
    '''
    Return the ratio of adjusted closing value to the simple moving average.
    INPUTS:
    values(pandas series)
    window(int): time period to consider
    OUTPUS:
    ratio(series)
    '''
    rm = values.rolling(window=window).mean()
    ratio = values/rm
    return ratio.apply(lambda x: round(x,5))

# Reference https://stackoverflow.com/questions/40256338/calculating-average-true-range-atr-on-ohlc-data-with-python

def wwma(values, n):
    return values.ewm(span=n, min_periods=n, adjust=False).mean()

def get_atr(df, n=14):
    data = df.copy()
    high = data['High']
    low = data['Low']
    close = data['Close']
    data['tr0'] = abs(high - low)
    data['tr1'] = abs(high - close.shift())
    data['tr2'] = abs(low - close.shift())
    tr = data[['tr0', 'tr1', 'tr2']].max(axis=1)
    atr = wwma(tr, n)
    return atr

get_atr(train_df)

def create_df(df, window=3):
    '''
    Create a dataframe with the normalized predictors
    norm_bb_width, norm_adj_close, norm_close_sma_ratio
    Input:
    df(dataframe)
    window(int): a window to compute rolling mean
    Ouput:
    df(dataframe): a new dataframe with normalized predictors
    '''
    
    # get bollinger value
    #bb_width = get_bollinger_bands(df['Adj Close'], window)
    # get the ratio of close price to simple moving average
    close_sma_ratio = get_adj_close_sma_ratio(df['Adj Close'], window)
    # get the upper and lower BB values 
    upper, lower = get_upper_lower_bands(df['Adj Close'], window)
    
    
    # create bb-width, close-sma-ratio columns
    #df['bb_width'] = bb_width
    df['close_sma_ratio'] = close_sma_ratio
    df['upper_bb'] = upper
    df['lower_bb'] = lower


    


    # drop missing values
    df.dropna(inplace=True)

    df['percent_b'] = (df['Adj Close'] - df['lower_bb'])*100 / (df['upper_bb'] - df['lower_bb'])

    #Calculate the Bollinger Percentage
    # numerator = difference(df['Adj Close'].tolist(), df['lower_bb'].tolist())
    # denominator = difference(df['upper_bb'].tolist(), df['lower_bb'].tolist())
    # ratio = [ b / m for b,m in zip(numerator, denominator)]
    # df['percent_b_1'] = [i * 100 for i in ratio]

    
    
    # normalize close price
    df['norm_adj_close'] = df['Adj Close']/df.iloc[0,:]['Adj Close']
    #df['norm_bb_width'] = df['bb_width']/df.iloc[0,:]['bb_width']
    df['norm_close_sma_ratio'] = df['close_sma_ratio']/df.iloc[0,:]['close_sma_ratio']
  
    return df

train_df = create_df(train_df, 5)

train_df.head()

plt.figure(figsize = (18,8))
plt.plot(train_df.index,train_df.loc[:,'Adj Close'], label = 'Adj Close' )
plt.plot(train_df.index, train_df.loc[:,'close_sma_ratio'], label = 'SMA')
plt.plot(train_df.index, train_df.loc[:,'upper_bb'], label = 'BBUpper' )
plt.plot(train_df.index, train_df.loc[:,'lower_bb'], label = 'BBLower')
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend(loc="upper left")
plt.show()

def discretize(values, num_states=9):
    '''
    Convert continuous values to integer state
    Inputs:
    values(pandas series)
    num_states(int): dividing the values in to n blocks
    Output:
    states_value(dict): a dictionary with state_value as key, and the real value as value
    '''
    states_value = dict()
    step_size = 1./num_states
    for i in range(num_states):
        if i == num_states - 1:
            states_value[i] = values.max()
        else:
            states_value[i] = values.quantile((i+1)*step_size)
    states_value[num_states] = float('inf') #3
    return states_value


def get_states(df):
    '''
    Discretize continous values to intergers
    Input:
    df(dataframe)
    Output:
    the discretized dictionary of norm_bb_width, 
    norm_adj_close, norm_close_sma_ratio columns
    '''
    # discretize values
    #price_states_value = discretize(df['norm_adj_close'])
    #bb_states_value = discretize(df['norm_bb_width'])
    close_sma_ratio_states_value = discretize(df['norm_close_sma_ratio'])
    
    return close_sma_ratio_states_value

close_sma_ratio_states_value = get_states(train_df)

close_sma_ratio_states_value

percent_b_states_values = {
0 : 0,
1 : 25,
2 : 75,
3 : 100,
4 : float('inf')
}

cash_list = [*range(1,10)]
cash_list = [int(180000/9)*each for each in cash_list]

cash_states_values = {}
for i in range(len(cash_list)):
  cash_states_values[i] = cash_list[i]
cash_states_values[9] = float("inf")

shares_list = [*range(1,10)]
shares_list = [int(252/9)*each for each in shares_list]

shares_states_values = {}
for i in range(len(shares_list)):
  shares_states_values[i] = shares_list[i]
shares_states_values[9] = float("inf")

def create_state_df(df, bb_states_value, close_sma_ratio_states_value):
    '''
    Add a new column to hold the state information to the dataframe
    Inputs:
    df(dataframe)
    price_states_value(dict)
    bb_states_value(dict)
    close_sma_ratio_states_value(dict)
    Output:
    df(dataframe)
    '''
    #df['norm_bb_width_state'] = df['norm_bb_width'].apply(lambda x : value_to_state(x, bb_states_value)) #2 
    df['norm_close_sma_ratio_state'] = df['norm_close_sma_ratio'].apply(lambda x : value_to_state(x, close_sma_ratio_states_value))
    df['percent_b_state'] = df['percent_b'].apply(lambda x : value_to_state(x, percent_b_states_values))
    #df['norm_adj_close_state'] = df['norm_adj_close'].apply(lambda x : value_to_state(x, price_states_value))
    
    #df['state'] = df['norm_close_sma_ratio_state'] + df['norm_bb_width_state']
    df['state'] = df['norm_close_sma_ratio_state'] + df['percent_b_state']
    df.dropna(inplace=True)
    return df

def value_to_state(value, states_value):
    '''
    Convert values to state
    Inputs:
    value(float)
    States_values(dict)
    Output:
    the converted state
    '''
    if np.isnan(value):
        return np.nan
    else:
        for state, v in states_value.items():
            if value <= v:
                return str(state)
        return 'value out of range'

def get_all_states(percent_b_states_values, close_sma_ratio_states_value, cash_states_values, shares_states_values):
    '''
    Combine all the states from the discretized 
    norm_adj_close, norm_close_sma_ratio columns.
    Inputs:
    price_states_value(dict)
    bb_states_value(dict)
    close_sma_ratio_states_value(dict)
    Output:
    states(list): list of strings
    '''
    states = []
    for c, _ in close_sma_ratio_states_value.items():
        for b, _ in percent_b_states_values.items():
          for m, _ in cash_states_values.items():
            for s, _ in shares_states_values.items(): 
              state =  str(c) + str(b) + str(m) + str(s)
              states.append(str(state))
    
    return states

train_df = create_state_df(train_df, percent_b_states_values , close_sma_ratio_states_value)

all_states = get_all_states(percent_b_states_values, close_sma_ratio_states_value, cash_states_values, shares_states_values)
states_size = len(all_states)
states_size
# test_df = create_df(test_df, 3)
# test_df = create_state_df(test_df, percent_b_states_values, close_sma_ratio_states_value)

train_df

def initialize_q_mat(all_states, all_actions):
    '''
    Initialize Q-table
    Inputs:
    all_states: a list of all the states values
    all_actions: a dictionary of all possible actions to take
    Output: 
    q_mat: randomly initialized Q-table
    '''
    states_size = len(all_states)
    actions_size = len(all_actions)
    
    q_mat = np.random.rand(states_size, actions_size)/1e9
    q_mat = pd.DataFrame(q_mat, columns=all_actions.keys())
    
    q_mat['states'] = all_states
    q_mat.set_index('states', inplace=True)
    
    return q_mat

def get_return_since_entry(bought_history, current_adj_close):
    '''
    Calculate the returns of current share holdings.
    Inputs:
    bought_history(list) 
    current_adj_close(float)
    current_day(int)
    Output:
    return_since_entry(float)
    '''
    return_since_entry = 0.
    
    for b in bought_history:
        return_since_entry += (current_adj_close - b)
    return return_since_entry

def act(state, q_mat, threshold=0.2, actions_size=3):
    '''
    Taking an action based on different strategies: 
    either random pick actions or take the actions 
    with the highest future return
    Inputs:
    state(str)
    q_mat(dataframe): Q-table
    threshold(float): the percentage of picking a random action
    action_size(int): number of possible actions 
    Output:
    action(int)
    '''
    if np.random.uniform(0,1) < threshold: # go random
        action = np.random.randint(low=0, high=actions_size)  
    else:
        action = np.argmax(q_mat.loc[state].values)
    return action

def get_return_since_entry(bought_history, current_adj_close):
    '''
    Calculate the returns of current share holdings.
    Inputs:
    bought_history(list) 
    current_adj_close(float)
    current_day(int)
    Output:
    return_since_entry(float)
    '''
    return_since_entry = 0.
    
    for b in bought_history:
        return_since_entry += (current_adj_close - b)
    return return_since_entry

train_df.head()

np.random.seed(12)
q = initialize_q_mat(all_states, all_actions)/1e5
print('Initializing q')
print(q[:10])

train_df[['Adj Close', 'state']].head()

0.8 * (x)^506 = 0.1

def train_q_learning(train_data, q, gamma, episodes):
    '''
    Train a Q-table 
    Inputs:
    train_data(dataframe)
    q(dataframe): initial Q-table
    alpha(float): threshold of which action strategy to take
    gamma(float): discount percentage on the future return
    Output:
    q(dataframe): Updated Q-table
    actions_history(dict): has everydays' actions and close price
    returns_since_entry(list): contains every day's return since entry
    '''
    # actions_history = []
    # num_shares = 0
    # bought_history = []
    # returns_since_entry = [0]
    # cash = 100000
    alpha = 0.4
    for ii in range(episodes):
        actions_history = []
        cash = 100000
        num_shares = 0
        if ii > 1:
          alpha = alpha*0.985
        epsilon = 0.8
        current_portfolio_value = []      
        for i, val in enumerate(train_data):
            current_adj_close, state = val
            try:
                next_adj_close, next_state = train_data[i+1]
            except:
                break


            current_cash_state = value_to_state(cash, cash_states_values)
            current_share_state = value_to_state(num_shares, shares_states_values)
            state = state + current_cash_state + current_share_state


            if i >=1:
              epsilon*= 0.9958
              
            action = act(state, q, threshold=epsilon, actions_size=3)
            
            # get reward
            if action == 0: # hold
                if num_shares > 0:
                    next_cash = cash # no change
                    reward = (cash + num_shares*next_adj_close) - (cash + num_shares*current_adj_close)                
                else:
                    reward = 0

            if action == 1: # buy
                if cash > current_adj_close:
                  next_cash = cash - current_adj_close
                  # reward = (cash - current_adj_close + ((num_shares+1)*next_adj_close)) - (cash + num_shares*current_adj_close)
                  reward = (next_cash + ((num_shares+1)*next_adj_close)) - (cash + num_shares*current_adj_close)
                  num_shares += 1
                  cash = next_cash
                else: 
                  reward = 0
            
            if action == 2: # sell
                if num_shares > 0:
                    next_cash = cash + current_adj_close
                    # reward = (cash + current_adj_close + ((num_shares-1)*next_adj_close)) - (cash + num_shares*current_adj_close)
                    reward = (next_cash + ((num_shares-1)*next_adj_close)) - (cash + num_shares*current_adj_close)
                    num_shares -= 1
                    cash = next_cash
                else:
                    reward = 0

            #NEXT using cash and share

            #next_cash_state = value_to_state(next_cash,cash_states_values)
            ## Use 'cash' instead as affect 'current'
            next_cash_state = value_to_state(cash,cash_states_values)
            next_share_state = value_to_state(num_shares, shares_states_values)
            ## Note: cash and num_share are automatically updated in at the end of the Action code block
            next_state = next_state + next_cash_state + next_share_state

            # #TODO 
            # Study 

            actions_history.append((i, current_adj_close, action))
            

            
            # update q table
            q.loc[state, action] = (1.-alpha)*q.loc[state, action] + alpha*(reward+gamma*(q.loc[next_state].max()))


            current_portfolio_value.append(cash + num_shares*next_adj_close)


    print('End of Training!')
    return q, actions_history, current_portfolio_value

def visualize_results(actions_history, returns_since_entry):
    '''
    Visualize the trading results with 2 plots
    The upper plot shows the return since entry
    The lower plot shows the action signal
    Inputs:
    actions_history(dict): has everydays' actions and close price
    returns_since_entry(list): contains every day's return since entry
    Output:
    None
    '''
    f, (ax1, ax2) = plt.subplots(2, 1, figsize=(30,24))
    
    ax1.plot(returns_since_entry)
    
    days, prices, actions = [], [], []
    for d, p, a in actions_history:
        days.append(d)
        prices.append(p)
        actions.append(a)

    #ax2.figure(figsize=(20,10))
    ax2.plot(days, prices, label='normalized adj close price')
    hold_d, hold_p, buy_d, buy_p, sell_d, sell_p = [], [], [], [], [], []
    for d, p, a in actions_history:
        if a == 0:
            hold_d.append(d)
            hold_p.append(p)
        if a == 1:
            buy_d.append(d)
            buy_p.append(p)
        if a == 2:
            sell_d.append(d)
            sell_p.append(p)
        # ax2.annotate(all_actions[a], xy=(d,p), xytext=(d-.2, p+0.001), color=color, arrowprops=dict(arrowstyle='->',connectionstyle='arc3'))
    ax2.scatter(hold_d, hold_p, color='blue', label='hold')
    ax2.scatter(buy_d, buy_p, color='green', label='buy')
    ax2.scatter(sell_d, sell_p, color='red', label='sell')
    ax2.legend()

def get_invested_capital(actions_history, returns_since_entry):
    '''
    Calculate the max capital being continously invested by the trader
    Input:
    actions_history(dict): has everydays' actions and close price
    returns_since_entry(list): contains every day's return since entry
    Output:
    return_invest_ratio(float)
    '''
    invest = []
    total = 0
    return_invest_ratio = None
    for i in range(len(actions_history)):
        a = actions_history[i][2]
        p = actions_history[i][1]

        try:
            next_a = actions_history[i+1][2]
        except:
            #print('end')
            break
        if a == 1:
            total += p
            #print(total)
            if next_a != 1 or (i==len(actions_history)-2 and next_a==1):
                invest.append(total)
                total = 0
    if invest:
        return_invest_ratio = returns_since_entry[-1]
        print('invested capital {}, return/invest ratio {}'.format(max(invest), return_invest_ratio))
    else:
        print('no buy transactions, invalid training')
    return return_invest_ratio

def get_base_return(data):
    '''
    Calculate the benchmark returns of a given stock
    Input:
    data(dataframe): containing normalized close price and state
    Output:
    return/invest ratio(float)
    '''
    start_price, _ = data[0]
    end_price, _ = data[-1]
    return (end_price - start_price)/start_price

def eval_q_learning(test_data, q):
    '''
    Evaluate the Q-table
    Inputs:
    test_data(dataframe)
    q(dataframe): trained Q-table
    Output:
    actions_history(dict): has everydays' actions and close price
    returns_since_entry(list): contains every day's return since entry
    '''
    actions_history = []
    current_portfolio_value = []
    cash = 100000
    num_shares = 0
    act_list = []
    for i, val in enumerate(test_data):
        current_adj_close, state = val
        try:
            next_adj_close, next_state = test_data[i+1]
        except:
            print('End of data! Done!')
            break   


        current_cash_state = value_to_state(cash, cash_states_values)
        current_share_state = value_to_state(num_shares, shares_states_values)
        state = state + current_cash_state + current_share_state

          
        action = act(state, q, threshold=0, actions_size=3)
        
        # get reward

        if action == 1: # buy
            if cash > current_adj_close:
              next_cash = cash - current_adj_close
              num_shares += 1
              cash = next_cash
            else: 
              action = 0
        
        if action == 2: # sell
            if num_shares > 0:
                next_cash = cash + current_adj_close
                num_shares -= 1
                cash = next_cash
            else:
                action = 0
        
        act_list.append(action)

        #NEXT using cash and share

        #next_cash_state = value_to_state(next_cash,cash_states_values)
        ## Use 'cash' instead as affect 'current'
        next_cash_state = value_to_state(cash,cash_states_values)
        next_share_state = value_to_state(num_shares, shares_states_values)
        ## Note: cash and num_share are automatically updated in at the end of the Action code block
        next_state = next_state + next_cash_state + next_share_state

        actions_history.append((i, current_adj_close, action))
        
        current_portfolio_value.append(cash + num_shares*next_adj_close)

    return actions_history, current_portfolio_value, act_list

pd.Series(train_returns_since_entry).describe()

pd.Series(train_actions_history).value_counts()

train_data = np.array(train_df[['norm_adj_close', 'state']])
q_mat, train_actions_history, train_returns_since_entry = train_q_learning(train_data, q, gamma=0.95, episodes=200)

q_mat[:10]

visualize_results(train_actions_history, train_returns_since_entry)
# get_invested_capital(train_actions_history, train_returns_since_entry)
# print('base return/invest ratio {}'.format(get_base_return(train_data)))

test_df = create_df(test_df, 5)
test_df = create_state_df(test_df, percent_b_states_values , close_sma_ratio_states_value)

test_data = np.array(test_df[['norm_adj_close', 'state']])
test_actions_history, test_returns_since_entry, act_list = eval_q_learning(test_data, q)

pd.Series(test_data[:,1]).value_counts()

visualize_results(test_actions_history, test_returns_since_entry)

pd.Series(test_returns_since_entry).describe()

train_return_invest_ratios = []
test_return_invest_ratios = []

list_eps = [*range(1,11)] + [10, 20, 30, 50, 100] 

for e in list_eps:
    np.random.seed(12)
    q = initialize_q_mat(all_states, all_actions)/1e9
    
    # train
    train_data = np.array(train_df[['norm_adj_close', 'state']])
    q, train_actions_history, train_returns_since_entry = train_q_learning(train_data, q, alpha=0.8, gamma=0.95, episodes=e)
    
    # print train results
    print('Training Results:')
    train_return_invest_ratio = get_invested_capital(train_actions_history, train_returns_since_entry)
    train_return_invest_ratios.append(train_return_invest_ratio)
    print('base return/invest ratio {}'.format(get_base_return(train_data)))
    
    # test
    print('Test Results:')
    test_actions_history, test_returns_since_entry = eval_q_learning(test_data, q)
    test_return_invest_ratio = get_invested_capital(test_actions_history, test_returns_since_entry)
    test_return_invest_ratios.append(test_return_invest_ratio)
    print('base return/invest ratio {}'.format(get_base_return(test_data)))

f, (ax1, ax2) = plt.subplots(2,1, sharex=True, figsize=(8,8))
x = [*range(1,11)] + [10,20, 30, 50, 100]
ax1.plot(x, train_return_invest_ratios, label='train', color='green');
ax2.plot(x, test_return_invest_ratios, label='test');
ax1.legend(loc=2);
ax2.legend(loc=2);
plt.xlabel('Train episode');
plt.ylabel('Return/Invest Ratio')

train_return_invest_ratios = []
test_return_invest_ratios = []
alphas = [i*0.05 for i in range(3, 10)]
for a in alphas:
    np.random.seed(12)
    q = initialize_q_mat(all_states, all_actions)/1e9
    
    # train
    train_data = np.array(train_df[['norm_adj_close', 'state']])
    q, train_actions_history, train_returns_since_entry = train_q_learning(train_data, q, alpha=a, gamma=0.95, episodes=4)
    
    # print train results
    print('Training Results:', end='')
    train_return_invest_ratio = get_invested_capital(train_actions_history, train_returns_since_entry)
    train_return_invest_ratios.append(train_return_invest_ratio)
    
    # test
    print('Test Results:', end='')
    test_actions_history, test_returns_since_entry = eval_q_learning(test_data, q)
    test_return_invest_ratio = get_invested_capital(test_actions_history, test_returns_since_entry)
    test_return_invest_ratios.append(test_return_invest_ratio)

f, (ax1, ax2) = plt.subplots(2,1, sharex=True, figsize=(8,8))
ax1.plot(alphas, train_return_invest_ratios, label='train', color='green');
ax2.plot(alphas, test_return_invest_ratios, label='test');
ax1.legend(loc=2);
ax2.legend(loc=2);
plt.xlabel('alpha');
plt.ylabel('Return/Invest Ratio')

start = time.time()
np.random.seed(12)
q = initialize_q_mat(all_states, all_actions)/1e9

# train
train_data = np.array(train_df[['norm_adj_close', 'state']])
q, train_actions_history, train_returns_since_entry = train_q_learning(train_data, q, alpha=0.35, gamma=0.95, episodes=4)
end = time.time()
print('Model trained in {:.3f}s'.format(end-start))

test_actions_history, test_returns_since_entry = eval_q_learning(test_data, q)
visualize_results(test_actions_history, test_returns_since_entry)
get_invested_capital(test_actions_history, test_returns_since_entry)
# print('invested capital {}, return/invest ratio {}'.format(invested_capital, return_invest_ratio))
print('base return/invest ratio {}'.format(get_base_return(test_data)))

start = datetime.datetime(2017, 11, 1)
end = datetime.datetime(2017, 12,1)
google_df = get_stock_data('GOOG', start, end, train_size=0.8)
google_df = create_df(google_df, 3)
google_df = create_state_df(google_df, percent_b_states_values, close_sma_ratio_states_value)

test_data = np.array(google_df[['norm_adj_close', 'state']])
g_actions_history, g_returns_since_entry = eval_q_learning(test_data, q)

visualize_results(g_actions_history, g_returns_since_entry)
get_invested_capital(g_actions_history, g_returns_since_entry)
print('base return/invest ratio {}'.format(get_base_return(test_data)))

start = datetime.datetime(2019, 1, 1)
end = datetime.datetime(2019, 12,1)
google_df = get_stock_data('JPM', start, end, train_size=0)
google_df = create_df(google_df, 3)
google_df = create_state_df(google_df,  percent_b_states_values, close_sma_ratio_states_value)

test_data = np.array(google_df[['norm_adj_close', 'state']])
g_actions_history, g_returns_since_entry = eval_q_learning(test_data, q)

visualize_results(g_actions_history, g_returns_since_entry)
get_invested_capital(g_actions_history, g_returns_since_entry)
print('base return/invest ratio {}'.format(get_base_return(test_data)))

Fs = 8000
f = 5
sample = 8000
x = np.arange(sample)
y = np.sin(2 * np.pi * f * x / Fs)

data = pd.DataFrame()
data['X'] = x
data['Adj Close'] = y

def get_stock_data(data, train_size=0.8):
    '''
    Get stock data in the given date range
    Inputs:
    symbol(str): stock symbol
    start(datetime): start date
    end(datetime): end date
    train_size(float): amount of data used for training
    Outputs:
    train_df, test_df OR df(if train_size=1)
    '''
    df = data
    
    train_len = int(df.shape[0] * train_size)
    
    if train_len > 0:
        train_df = df.iloc[:train_len, :]
        test_df = df.iloc[train_len:, :]
        return train_df, test_df
    else:
        return df

google_df = get_stock_data(data, train_size=0)
google_df = create_df(google_df, 3)
google_df = create_state_df(google_df,  bb_states_value, close_sma_ratio_states_value)

test_data = np.array(google_df[['norm_adj_close', 'state']])
g_actions_history, g_returns_since_entry = eval_q_learning(test_data, q)

test_data

visualize_results(g_actions_history, g_returns_since_entry)
get_invested_capital(g_actions_history, g_returns_since_entry)
print('base return/invest ratio {}'.format(get_base_return(test_data)))

